inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082
number_of_netty_threads=4
job_queue_size=100
model_store=/home/model-server/model-store
load_models=all
default_workers_per_model=2
max_request_size=10485760
max_response_size=10485760
# Disable token authentication for inference
disable_token_authorization=true
enable_envvars_config=true
enable_metrics_api=true
metrics_format=json